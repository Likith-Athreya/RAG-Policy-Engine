RAG Policy Engine

A small Retrieval-Augmented Generation (RAG) system that answers questions strictly based on internal company policy documents such as Refund Policy, Shipping Policy, and Terms of Service.

This project was built as part of an AI Engineer intern take-home assignment. The focus is on prompt design, retrieval correctness, and hallucination avoidance rather than UI or scale.

What This Project Does

Loads company policy documents (PDFs)

Splits them into meaningful text chunks

Generates embeddings and stores them in a vector database

Retrieves relevant context for each query

Generates answers only from retrieved context

Avoids guessing when information is missing

Two prompt versions are used to compare output quality and safety.

Tech Stack

Python

LangChain

Chroma (vector store)

Ollama (nomic-embed-text) for embeddings

Groq-hosted LLM (LLaMA-based)

How It Works

Policy PDFs are loaded from the docs/ directory

Text is split into overlapping chunks

Each chunk is embedded and stored in Chroma

For each query:

Top-k relevant chunks are retrieved

Retrieved context is passed to the LLM via a prompt

The model generates an answer using only that context

Chunking Strategy
chunk_size = 600
chunk_overlap = 100


This was chosen because:

Policy clauses often span multiple sentences

Larger chunks preserve meaning better than very small splits

Overlap prevents losing information at chunk boundaries

The goal is factual accuracy, not summarization

Prompt Design
Prompt Version 1 (Baseline)
Answer based on context: {context}
Question: {input}


This prompt allows fluent answers but does not strongly restrict the model from adding assumptions.

Prompt Version 2 (Improved)
You are a professional Policy Assistant.
Use ONLY the provided context to answer.
If the answer is not in the context, say "I do not have sufficient information."
Format your answer using bullet points.


Changes made:

Enforced strict grounding to retrieved context

Added a clear fallback for missing information

Improved readability using structured output

Significantly reduced hallucinations

Evaluation

Five test questions were used, including:

Fully answerable questions

Questions requiring reasoning across policy sections

Legal and delivery-related edge cases

Questions that could cause hallucination if not handled carefully

Result:

Prompt Version 2 produced clearer, safer, and more policy-faithful answers than Prompt Version 1

Edge Case Handling

If no relevant information is retrieved, the system responds with:
I do not have sufficient information.

Out-of-scope questions are not answered speculatively

All answers remain tied directly to document content

Trade-offs

No UI, to keep focus on reasoning and prompt quality

No reranking step, for simplicity and transparency

Local embeddings chosen to avoid API dependency and cost

What I’d Improve With More Time

Add a reranking layer for better retrieval precision

Persist the vector database between runs

Add automated evaluation metrics

Enforce structured output using a schema

Repository Structure
docs/
├── Refund_policy.pdf
├── logistics_policy.pdf
└── terms_service.pdf

lang.py
requirements.txt
README.md

Notes

All results shown were generated by running the script locally.
This README reflects the actual behavior of the system.
